# container serving a LLM locally
Basic API serving 'small llm' (slm?)
Plan is to use 
+ https://huggingface.co/distilbert/distilgpt2  (License Apache 2)
+ maybe one of the https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3	



