# container serving a LLM locally
Basic API serving 'small llm' (slm?)

This branch uses https://huggingface.co/distilbert/distilgpt2  (License Apache 2)

# build image
tested with podman --> Works for me (tm)

# run image
see http://localhost:5000/docs

IMPORTANT: first run will download the model from huggingface, it will take 'some' time


